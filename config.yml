---

model: quartznet5x5  # ["quartznet5x5", "quartznet10x5", "quartznet15x5"]

# Datasets
train:
  data_list: assets/encoder_train_data.txt
  data_dirs: ["LibriTTS/train_clean"]
  apply_speed_pertrubation: True
  apply_masking: True

val:
  data_list: assets/encoder_val_data.txt
  data_dirs: ["LibriTTS/test_clean"]

test:
  weights: checkpoints/encoder/model.pt
  data_list: assets/encoder_test_data.txt
  data_dirs: [ "LibriTTS/test_clean" ]

# Training
max_length: 12  # maximum file length in seconds. Can be used to avoid OOM (out-of-memory) error
epochs: 3
batch_size: 1
learning_rate: 5e-4  # constant learning rate value. Omitted if using OneCycleLR
weight_decay: 0.0001
checkpoint_dir: checkpoints/encoder
log_dir: logs/encoder
use_onecyclelr: True

# OneCycleLR parameters
# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR
max_lr: 1e-3
div_factor: 25.0
pct_start: 0.3

# Augmentation
speed_pertrubation: 0.1
masking:
  chunk_size: 30  # size of each spectrogram segment (time axis) to apply augmentation to. Set equal to -1 to apply original pytorch's masking
  freq_masking: 10
  time_masking: 6

spec_params:
  sr: 16000
  n_mels: 80
  n_fft: 1024
  win_length: 1024
  hop_length: 256

stats: assets/stats.npy  # file with the mean and std of the data